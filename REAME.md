# **Analysis of a Simulated Annealing-Inspired Approach for Patent Query Optimization**

**专利查询优化的模拟退火启发式方法分析**

**1\. Introduction  1. 引言**  
Effective patent searching presents a significant challenge due to the sheer volume of available literature and the complexity of articulating precise and comprehensive queries. The ability to efficiently retrieve relevant prior art is crucial for various stakeholders, including inventors, researchers, and legal professionals. Traditional methods of manual query formulation often rely on expert knowledge and can be time-consuming and may not always yield optimal results in terms of recall and precision. To address these limitations, automated techniques for optimizing patent search queries have gained increasing attention. This report analyzes a Python-based approach that leverages a simulated annealing-inspired methodology to refine search queries for patent documents. The code under examination outlines a workflow encompassing data preprocessing, the creation of a search index, the selection of relevant keywords based on TF-IDF (Term Frequency-Inverse Document Frequency), and an optimization strategy, initially intended to be simulated annealing, but ultimately implemented as a greedy iterative process, to construct effective search queries. This report will delve into the various components of this approach, examining its methodology, potential strengths, limitations, and offering suggestions for further development.  
有效专利搜索面临着重大挑战，因为现有文献数量庞大，且精确全面地表述查询复杂。高效检索相关现有技术对于包括发明者、研究人员和法律专业人士在内的各方至关重要。传统的手动查询制定方法通常依赖于专家知识，可能耗时且不一定总是能在召回率和精确度方面产生最佳结果。为了解决这些局限性，针对优化专利搜索查询的自动化技术越来越受到关注。本报告分析了一种基于 Python 的方法，该方法利用模拟退火启发式方法来优化专利文档的搜索查询。审查的代码概述了一个包括数据预处理、创建搜索索引、基于 TF-IDF（词频-逆文档频率）选择相关关键词以及一个优化策略的工作流程，最初旨在实现模拟退火，但最终实现为贪婪迭代过程，以构建有效的搜索查询。 本报告将深入探讨该方法的各个组成部分，检查其方法、潜在优势、局限性，并提出进一步发展的建议。  
**2\. Code Description and Workflow**  
**2\. 代码描述和工作流程**  
The provided Python code implements a pipeline for optimizing patent search queries, primarily focusing on constructing "OR" queries from a set of candidate keywords. The workflow can be broken down into several key stages, starting with environment setup and data loading, followed by index creation, and finally, the query optimization process.  
该提供的 Python 代码实现了一个用于优化专利搜索查询的管道，主要关注从一组候选关键词中构建“或”查询。工作流程可以分解为几个关键阶段，从环境设置和数据加载开始，然后是索引创建，最后是查询优化过程。  
**2.1. Environment Setup and Library Installation**  
**2.1. 环境设置和库安装**  
The initial step in the provided code involves installing the Whoosh library using pip and a local wheel file \[None\]. Whoosh is a pure-Python library designed for indexing and searching textual content. The fact that a specific wheel file is used for installation suggests a deliberate choice, possibly to ensure compatibility with a particular version of the library or due to constraints within the execution environment, such as the Kaggle platform where this code originates. This explicit version control helps maintain reproducibility and avoids potential issues that might arise from using the latest version, which could introduce breaking changes or incompatibilities with other dependencies.  
提供的代码中的第一步涉及使用 pip 和本地 wheel 文件 \[None\] 安装 Whoosh 库。 Whoosh 是一个纯 Python 库，用于索引和搜索文本内容。使用特定 wheel 文件进行安装的事实表明这是一种故意的选择，可能是为了确保与库的特定版本兼容，或者由于执行环境中的限制，例如代码起源于 Kaggle 平台。这种显式的版本控制有助于保持可重复性，并避免使用最新版本可能带来的潜在问题，因为最新版本可能会引入破坏性更改或不兼容其他依赖项。  
**2.2. Data Loading and Preprocessing**  
**2.2. 数据加载和预处理**  
The subsequent code blocks focus on loading and preprocessing patent data using the polars library \[None\]. Polars is known for its efficiency in handling large datasets, making it suitable for processing the potentially extensive patent literature. The code first reads patent metadata, including publication dates and CPC (Cooperative Patent Classification) codes. It then extracts the year and month of publication. While a filter for patents published since 1975 is present, it is commented out in the provided code. The metadata is then filtered to include only those patents whose publication numbers appear in a separate test set. This step ensures that the subsequent index and query optimization are focused on the patents relevant to the evaluation task. Following the metadata filtering, the code proceeds to load the actual patent text data, specifically excluding the "claims" and "description" fields initially. These fields are later added back as empty strings to the patent DataFrame. This might indicate a strategy where the initial indexing and keyword selection focus on other fields like the title or abstract, with the possibility of incorporating the claims and description later, or it could imply that these fields are not used in the current query optimization process. The metadata and patent text data are then joined based on the "publication\_number," and the combined data is saved as a Parquet file named "meta\_with\_text.parquet." The use of polars's lazy evaluation capabilities (scan\_parquet, lazy, collect(streaming=True)) is noteworthy for its potential to handle large datasets efficiently by optimizing the query execution plan before actual computation.  
下面的代码块专注于使用 polars 库\[无\]加载和预处理专利数据。 Polars 以其处理大型数据集的效率而闻名，这使得它非常适合处理可能广泛的专利文献。代码首先读取专利元数据，包括出版日期和 CPC（合作专利分类）代码。然后提取出版年份和月份。虽然存在一个自 1975 年以来出版的专利过滤器，但在提供的代码中已被注释掉。然后对元数据进行过滤，仅包括那些出版号出现在单独测试集中的专利。这一步骤确保后续的索引和查询优化集中在与评估任务相关的专利上。在元数据过滤之后，代码继续加载实际的专利文本数据，最初排除“权利要求”和“描述”字段。这些字段稍后作为空字符串添加回 patent DataFrame 中。 这可能表明一种策略，其中初始索引和关键词选择侧重于其他字段，如标题或摘要，以后再可能包含权利要求书和描述，或者这可能意味着这些字段在当前的查询优化过程中没有被使用。然后根据“出版号”将元数据和专利文本数据连接起来，并将合并后的数据保存为名为“meta\_with\_text.parquet”的 Parquet 文件。使用 polars 的延迟评估能力（ scan\_parquet 、 lazy 、 collect(streaming=True) ）值得关注，因为它通过在实际计算之前优化查询执行计划，能够有效地处理大型数据集。  
**2.3. Index Creation  2.3. 索引创建**  
Once the data is preprocessed, the code proceeds to create a search index using the Whoosh library \[None\]. The processed metadata with text is converted into a list of dictionaries, where each dictionary represents a patent document. These documents are then used by the whoosh\_utils.create\_index function to build an index in a directory named "test\_index." This index allows for efficient searching of the patent data based on the fields indexed by Whoosh. The whoosh\_utils module likely contains custom functions that simplify the interaction with the Whoosh library, such as defining the schema of the index and adding documents to it. This indexing step is crucial as it enables the rapid retrieval of patent documents based on the generated search queries during the optimization process.  
数据预处理完成后，代码将使用 Whoosh 库创建搜索索引\[无\]。处理后的包含文本的元数据被转换为字典列表，每个字典代表一份专利文档。然后，这些文档被 whoosh\_utils.create\_index 函数用于在名为“test\_index”的目录中构建索引。此索引允许基于 Whoosh 索引的字段高效搜索专利数据。 whoosh\_utils 模块可能包含简化与 Whoosh 库交互的定制函数，例如定义索引模式以及向其中添加文档。此索引步骤至关重要，因为它使得根据优化过程中生成的搜索查询快速检索专利文档成为可能。  
**2.4. Simulated Annealing Implementation**  
**2.4. 模拟退火实现**  
The code includes an implementation of a generic Annealer class, which provides the framework for performing simulated annealing \[None\]. Simulated annealing is a metaheuristic optimization algorithm inspired by the physical process of annealing in materials. It works by iteratively exploring the search space of possible solutions, probabilistically accepting moves to neighboring states, even if those moves worsen the objective function. This probabilistic acceptance helps the algorithm escape local optima and potentially find a globally optimal solution. The Annealer class defines essential methods such as move (to generate a new state), energy (to evaluate the quality of a state), and anneal (to run the optimization process based on a defined temperature schedule). The parameters of the annealing schedule, such as the initial temperature (Tmax), the final temperature (Tmin), the number of steps, and the maximum time allowed, control the exploration-exploitation trade-off of the algorithm. A higher initial temperature allows for more exploration of the search space, while a slower cooling rate (determined by the temperature reduction schedule) allows for more exploitation of promising regions.  
代码包含一个通用的 Annealer 类实现，该类提供了执行模拟退火\[无\]的框架。模拟退火是一种受材料退火物理过程启发的元启发式优化算法。它通过迭代探索可能解的搜索空间，以概率接受移动到相邻状态，即使这些移动会恶化目标函数。这种概率接受有助于算法逃离局部最优，并可能找到全局最优解。 Annealer 类定义了基本方法，如 move （生成新状态）、 energy （评估状态质量）和 anneal （根据定义的温度计划运行优化过程）。退火计划参数，如初始温度（ Tmax ）、最终温度（ Tmin ）、步数和允许的最大时间，控制算法的探索-利用权衡。较高的初始温度允许对搜索空间进行更多探索，而较慢的冷却速率（由温度降低计划确定）允许对有希望的领域进行更多利用。  
**2.5. Helper Functions and Data Structures**  
**2.5. 辅助函数和数据结构**  
Several helper functions and data structures are defined to support the query optimization process \[None\]. The select\_top\_k\_columns function takes a matrix (presumably a TF-IDF matrix) and selects the top-k columns based on the sum of values in each column. This function is used to identify the most important terms based on their TF-IDF scores. The ap50 function calculates the Average Precision at 50, a metric commonly used to evaluate the performance of information retrieval systems. It measures the average precision over the top 50 retrieved documents. A comment within the code notes a potential issue with the correctness of this implementation for the specific competition's scoring, which is a critical point to consider when interpreting the results. The Word dataclass is used to represent a word along with its category (e.g., "ti" for title, "cpc" for CPC code). The State dataclass represents a potential query as a collection of Word objects and a boolean array indicating whether each word is included in the query. The move\_1 method within the State class defines a simple move operation by randomly toggling the inclusion of a word in the query. The to\_query method constructs the actual "OR" query string based on the words selected in the current state.  
定义了多个辅助函数和数据结构以支持查询优化过程\[无\]。 select\_top\_k\_columns 函数接收一个矩阵（可能是 TF-IDF 矩阵）并根据每列值的总和选择前 k 列。此函数用于根据 TF-IDF 分数识别最重要的术语。 ap50 函数计算 50 的平均精度，这是一个常用于评估信息检索系统性能的指标。它测量前 50 个检索文档的平均精度。代码中的注释指出，此实现对于特定竞赛评分的正确性可能存在问题，这是在解释结果时需要考虑的关键点。 Word 数据类用于表示一个词及其类别（例如，“ti”表示标题，“cpc”表示 CPC 代码）。 State 数据类表示一个潜在查询，它是一个 Word 对象的集合和一个布尔数组，指示每个词是否包含在查询中。 State 类中的 move\_1 方法定义了一个简单的移动操作，通过随机切换词在查询中的包含状态。 第 7 号方法根据当前状态中选定的单词构建实际的“OR”查询字符串。  
**2.6. Loading Additional Data and Models**  
**2.6. 加载额外的数据和模型**  
The code then proceeds to load additional data and pre-trained models \[None\]. This includes the test set, the pre-processed metadata, the Whoosh index created earlier, and pre-trained TF-IDF models for patent titles (ti\_tfidf) and CPC codes (cpc\_cv\_tfidf). The loading of these pre-trained TF-IDF models suggests that they were trained on a larger corpus of patent data and are used here to identify important terms within the titles and CPC codes of the patents in the test set. The identity function is used as a tokenizer for the TF-IDF vectorizers, indicating that the input text is either already tokenized or that single terms are being considered as features. The availability of these pre-trained models and the pre-built index streamlines the optimization process by avoiding the need to train these models or build the index in every run.  
代码随后加载额外的数据和预训练模型\[无\]。这包括测试集、预处理后的元数据、之前创建的 Whoosh 索引，以及用于专利标题（ ti\_tfidf ）和 CPC 代码（ cpc\_cv\_tfidf ）的预训练 TF-IDF 模型。这些预训练 TF-IDF 模型的加载表明，它们是在更大的专利数据语料库上训练的，并且在这里用于识别测试集中专利标题和 CPC 代码中的重要术语。 identity 函数用作 TF-IDF 向量化器的分词器，表示输入文本已经分词或单个术语被视为特征。这些预训练模型和预建索引的存在通过避免每次运行都需要训练这些模型或构建索引的需要，简化了优化过程。  
**2.7. Main Optimization Loop**  
**2.7. 主要优化循环**  
The core of the query optimization process lies within the main loop, which iterates through each publication number in the test set \[None\]. For each publication number, the code retrieves the target relevant patents and their metadata. If no metadata is found for a given publication number, a dummy query ("ti:device") is appended to the results. For the cases where metadata is available, the titles and CPC codes of the relevant patents are transformed into TF-IDF vectors using the pre-trained models. The select\_top\_k\_columns function is then used to identify the top 80 words based on their TF-IDF scores for both titles and CPC codes. The code initially includes a commented-out section that appears to set up the initial state for the simulated annealing algorithm using these top-k words. However, the subsequent implementation deviates from this approach. Instead of directly using the Annealer class, the code iterates through each of the top-k words (both from titles and CPC codes) individually. For each word, it constructs a single-term query and evaluates its performance using the ap50 metric. The words are then sorted based on their individual AP@50 scores in descending order. Following this, the code iteratively builds a query by adding the top-scoring words one by one, forming an "OR" query. At each step, the AP@50 score of the combined query is evaluated. A word is only kept in the query if adding it improves the AP@50 score or maintains the current best score. The process also stops if the number of tokens in the query reaches a limit of 50\. Finally, an additional iterative step is performed where words are removed one by one from the constructed query to see if this further improves the AP@50 score. The publication number and the resulting optimized query are then stored in the results list, and the AP@50 score for that query is recorded. After processing all publication numbers in the test set, the average AP@50 score is calculated and printed. This shift from the intended simulated annealing approach to a greedy iterative method suggests a potential reason, perhaps related to the computational cost or the effectiveness of the initial simulated annealing implementation. The greedy approach, while less computationally intensive, might be more susceptible to getting stuck in local optima. The iterative refinement step of removing words indicates an attempt to mitigate this limitation.  
查询优化过程的核心在于主循环，该循环遍历测试集中的每个出版物编号\[无\]。对于每个出版物编号，代码检索目标相关专利及其元数据。如果找不到给定出版物编号的元数据，则将一个虚拟查询("ti:device")附加到结果中。对于有元数据的案例，使用预训练模型将相关专利的标题和 CPC 代码转换为 TF-IDF 向量。然后使用 select\_top\_k\_columns 函数根据标题和 CPC 代码的 TF-IDF 分数识别前 80 个单词。代码最初包含一个注释掉的段落，看起来是使用这些前 k 个单词设置模拟退火算法的初始状态。然而，随后的实现偏离了这种方法。代码不是直接使用 Annealer 类，而是逐个遍历前 k 个单词（来自标题和 CPC 代码）。对于每个单词，它构建一个单词查询并使用 ap50 指标评估其性能。然后根据单词的 AP@50 分数按降序排序。 随后，代码通过逐个添加得分最高的单词来迭代构建查询，形成一个“或”查询。在每一步，都会评估组合查询的 AP@50 分数。只有当添加一个单词能提高 AP@50 分数或保持当前最佳分数时，该单词才会保留在查询中。如果查询中的标记数达到 50 个限制，则停止该过程。最后，执行一个额外的迭代步骤，逐个从构建的查询中删除单词，以查看这是否能进一步提高 AP@50 分数。然后，将出版物编号和生成的优化查询存储在 results 列表中，并记录该查询的 AP@50 分数。处理完测试集中的所有出版物编号后，计算并打印平均 AP@50 分数。这种从预期的模拟退火方法到贪婪迭代方法的转变，可能是一个潜在的原因，可能与计算成本或初始模拟退火实现的有效性有关。贪婪方法虽然计算量较小，但可能更容易陷入局部最优。 迭代去除单词的步骤表明了一种减轻这种限制的尝试。  
**2.8. Submission File Generation**  
**2.8. 提交文件生成**  
The final steps involve removing any unwanted files and directories from the working environment and generating a submission CSV file containing the publication numbers and their corresponding optimized queries \[None\]. This file is then used for submitting the results to the competition or for further evaluation.  
最后的步骤包括从工作环境中删除任何不需要的文件和目录，并生成一个包含出版号及其对应优化查询的提交 CSV 文件。此文件随后用于提交竞赛结果或进一步评估。  
**3\. In-depth Analysis of the Implemented Greedy Approach (and the Intended Simulated Annealing)**  
**3\. 对实现贪婪算法（以及预期的模拟退火）的深入分析**  
While the code initially outlines the use of simulated annealing for query optimization, the main execution loop implements a greedy approach with a subsequent refinement step. It is still valuable to analyze the components of the intended simulated annealing framework before focusing on the implemented strategy.  
虽然代码最初概述了使用模拟退火进行查询优化的用途，但主要执行循环实现了贪婪算法，并随后进行细化。在关注实现策略之前，分析预期模拟退火框架的组件仍然很有价值。  
**3.1. State Representation (Intended)**  
**3.1. 状态表示（预期）**  
The State class was designed to represent a potential query as a list of Word objects and a corresponding boolean array. Each element in the boolean array indicates whether the word at the same index in the words list should be included in the query. This representation provides a flexible way to explore different combinations of keywords. The initial state of the system is created by randomly assigning a 0 or 1 to each element in the boolean array, effectively starting with a random subset of the candidate words.  
State 类被设计用来将一个潜在查询表示为 Word 对象的列表和相应的布尔数组。布尔数组中的每个元素指示 words 列表中相同索引处的单词是否应包含在查询中。这种表示方法提供了一种灵活的方式来探索不同的关键词组合。系统的初始状态通过随机分配布尔数组中每个元素的 0 或 1 来创建，实际上是从候选单词的随机子集开始。  
**3.2. Move Function (Intended)**  
**3.2. 移动函数（预期）**  
The move\_1 function within the State class defines a simple move operation. It randomly selects a word from the list and toggles its inclusion status in the query (from included to excluded or vice versa). This simple move operation defines the neighborhood structure of the search space, where neighboring states differ by the inclusion status of a single word. More complex move operations could have been implemented, such as swapping words or changing multiple words at once, which might have allowed for a more efficient exploration of the search space.  
State 类中的 move\_1 函数定义了一个简单的移动操作。它从列表中随机选择一个单词，并切换其在查询中的包含状态（从包含到排除或反之亦然）。这种简单的移动操作定义了搜索空间中的邻域结构，其中相邻状态的不同之处在于单个单词的包含状态。可以实施更复杂的移动操作，例如交换单词或同时更改多个单词，这可能会允许更有效地探索搜索空间。  
**3.3. Energy Function (Intended)**  
**3.3. 能量函数（预期）**  
The energy function in the USPTOProblem class calculates the negative AP@50 score of the query generated by the current state. The negative score is used because simulated annealing is typically formulated as a minimization problem. Minimizing the negative AP@50 score is equivalent to maximizing the AP@50 score, which is the ultimate goal of the query optimization process. The choice of AP@50 as the evaluation metric directly influences the type of queries that the algorithm will consider optimal.  
energy 类中的 USPTOProblem 函数计算当前状态下生成的查询的负 AP@50 分数。使用负分数是因为模拟退火通常被表述为最小化问题。最小化负 AP@50 分数等同于最大化 AP@50 分数，这是查询优化过程的最终目标。直接选择 AP@50 作为评估指标将直接影响算法认为最优的查询类型。  
**3.4. Annealing Schedule (Intended)**  
**3.4. 退火计划（预期）**  
The USPTOProblem class allows for setting various parameters related to the annealing schedule, such as the maximum temperature (Tmax), the minimum temperature (Tmin), the number of steps, and the maximum time allowed for the optimization. These parameters control the behavior of the simulated annealing algorithm. A higher initial temperature allows for a greater probability of accepting moves that worsen the energy (lower AP@50 score), enabling the algorithm to explore a wider range of solutions and potentially escape local optima. As the temperature decreases over time, the probability of accepting worsening moves decreases, and the algorithm starts to converge towards a local optimum. The number of steps and the maximum time limit the duration of the optimization process. The commented-out nature of the USPTOProblem instantiation and the anneal method call in the main loop suggests that the initial attempt to use simulated annealing might have been abandoned in favor of the implemented greedy approach. This could have been due to factors such as the computational cost associated with running simulated annealing for each query, the perceived effectiveness of the results obtained, or challenges in tuning the annealing parameters appropriately.  
USPTOProblem 类允许设置与退火计划相关的各种参数，例如最大温度（ Tmax ）、最小温度（ Tmin ）、步数以及优化允许的最大时间。这些参数控制模拟退火算法的行为。较高的初始温度允许接受使能量变差（AP@50 分数较低）的移动的可能性，使算法能够探索更广泛的解决方案，并可能逃离局部最优。随着时间的推移，温度降低，接受变差移动的概率降低，算法开始收敛到局部最优。步数和最大时间限制优化过程的持续时间。主循环中 USPTOProblem 实例化和 anneal 方法调用的注释性质表明，最初尝试使用模拟退火可能已被放弃，转而采用实现的贪婪方法。 这可能是因为与为每个查询运行模拟退火相关的计算成本、获得的结果的有效性感知，或者调整退火参数的挑战等因素。  
**3.5. Analysis of the Implemented Greedy Approach**  
**3.5. 实现的贪婪算法分析**  
The implemented approach deviates significantly from the intended simulated annealing strategy. It begins by evaluating the individual performance (AP@50 score) of each of the top-k words identified from the titles and CPC codes using TF-IDF. This initial evaluation provides a ranking of the candidate keywords based on their individual ability to retrieve relevant patents. The approach then greedily builds the query by iteratively adding the highest-scoring words, forming an "OR" combination. At each step, the AP@50 score of the current query is evaluated. A word is added to the query only if it improves the AP@50 score or maintains the best score achieved so far. This greedy strategy aims to build a query that maximizes the AP@50 score by selecting the most promising individual terms. However, a limitation of this approach is that it might not capture synergistic effects between words. A combination of two or more words that individually have lower scores might collectively result in a higher AP@50 score than a query built solely from the highest-scoring individual words. The constraint on the maximum number of tokens (50) in the query is also enforced during this process. After the greedy construction phase, the code performs an additional refinement step where it iteratively removes each word from the constructed query and evaluates the resulting AP@50 score. If removing a word leads to an improvement in the score, the word is permanently removed. This step attempts to address some of the limitations of the purely greedy approach by exploring whether removing certain words from the initially selected set can further enhance the query's performance. This suggests that simply adding the top-scoring individual terms might not always lead to the optimal "OR" query, and some terms might introduce noise or redundancy.  
实施的方法与预期的模拟退火策略有显著差异。它首先使用 TF-IDF 评估从标题和 CPC 代码中识别出的前 k 个单词的个体性能（AP@50 分数）。这种初步评估为候选关键词提供了一个基于它们各自检索相关专利能力的排名。然后，该方法通过迭代添加得分最高的单词，形成“或”组合，贪婪地构建查询。在每一步，都会评估当前查询的 AP@50 分数。只有当单词添加到查询中能提高 AP@50 分数或保持迄今为止的最佳分数时，才会将其添加到查询中。这种贪婪策略旨在通过选择最有希望的个体术语来构建一个最大化 AP@50 分数的查询。然而，这种方法的一个局限性是它可能无法捕捉到单词之间的协同效应。两个或更多单词的组合，其个体分数较低，可能集体产生比仅从得分最高的个体单词构建的查询更高的 AP@50 分数。在查询过程中，也强制执行查询中最大标记数（50）的限制。 在贪婪构建阶段之后，代码执行一个额外的细化步骤，其中它迭代地从构建的查询中移除每个单词并评估结果 AP@50 分数。如果移除一个单词导致分数提高，则该单词将被永久移除。这一步骤试图通过探索从最初选择的集合中移除某些单词是否可以进一步提高查询性能来解决纯贪婪方法的一些局限性。这表明，简单地添加得分最高的单个术语并不总是能导致最优的“或”查询，某些术语可能引入噪声或冗余。  
**4\. Evaluation of the AP@50 Metric**  
**4\. AP@50 指标评估**  
The Average Precision at 50 (AP@50) is used as the primary metric to evaluate the effectiveness of the generated patent search queries. This metric focuses on the precision of the top 50 retrieved documents. For each relevant document within the top 50 results, the precision at that rank is calculated, and the AP@50 score is the average of these precision values. This metric is commonly used in information retrieval to assess how well a search query ranks relevant documents at the top of the result list. In the context of patent search, where users often examine only the first few pages of results, AP@50 provides a measure of how likely a user is to find relevant patents within the initial set of retrieved documents. However, it's important to consider whether AP@50 adequately captures all the requirements of patent searching. For instance, in prior art searches, the recall of all relevant documents, even if they are not within the top 50, can be crucial. The comment in the code regarding the potential incorrectness of the ap50 implementation for the competition's scoring raises a significant concern. If the metric is not being calculated correctly according to the competition's rules, the scores obtained and used to guide the query optimization process might not accurately reflect the true performance of the generated queries. This could lead to the selection of suboptimal queries based on a flawed evaluation.  
平均精度 AP@50（Average Precision at 50）被用作评估生成专利检索查询有效性的主要指标。该指标关注前 50 条检索文档的精度。对于前 50 条结果中的每个相关文档，计算该排名的精度，AP@50 分数是这些精度值的平均值。该指标在信息检索中常用于评估搜索查询如何将相关文档排名在结果列表的顶部。在专利检索的背景下，用户通常只查看结果的前几页，AP@50 提供了用户在检索文档的初始集中找到相关专利的可能性的度量。然而，考虑 AP@50 是否充分捕捉到专利检索的所有要求是很重要的。例如，在现有技术检索中，即使相关文档不在前 50 条中，所有相关文档的召回率也可能至关重要。关于竞赛评分中 ap50 实现可能不正确的代码注释提出了一个重大担忧。 如果指标未根据比赛规则正确计算，那么获得的分数以及用于指导查询优化过程的分数可能无法准确反映生成的查询的真实性能。这可能导致基于有缺陷的评估选择次优查询。  
**5\. Discussion of Results and Potential Limitations**  
**5\. 结果讨论和潜在局限性**  
The reported average AP@50 score provides a quantitative measure of the effectiveness of the queries generated by the implemented greedy approach on the test set. To fully understand the significance of this score, it would be beneficial to compare it with baseline scores obtained using simpler query strategies or the performance reported in the original Kaggle notebook that inspired this code.  
报告的平均 AP@50 分数提供了对在测试集上通过实现的贪婪方法生成的查询有效性的定量度量。为了充分理解这个分数的意义，将其与使用更简单的查询策略获得的基线分数或原始 Kaggle 笔记本中报告的性能进行比较将是有益的。

| Method  方法 | Average AP@50  平均 AP@50 |
| :---- | :---- |
| Implemented Greedy Approach 实现的贪婪方法 |  |
| Simple Keyword Search Baseline 简单关键词搜索基线 |  |
| Original Notebook Baseline 原始笔记本基线 |  |

The current approach, while showing some level of automation in query optimization, exhibits several potential limitations. The restriction to constructing queries using only the "OR" operator limits the expressiveness of the search queries. Patent language often involves complex relationships between different concepts, which might be better captured using Boolean operators like "AND" or proximity operators. The reliance on TF-IDF for selecting candidate keywords, while a common technique, might not always identify the most semantically relevant terms in the context of patent search. Terms with high TF-IDF scores might be frequent in the patent corpus but not necessarily the most discriminative for a specific search task. The implemented greedy optimization strategy, as discussed earlier, might get stuck in local optima and fail to find globally optimal combinations of keywords. The order in which words are added to the query can significantly impact the final result. Furthermore, the potential issues with the ap50 implementation could lead to an inaccurate evaluation of the query performance, hindering the optimization process. The approach primarily focuses on keywords extracted from the title and CPC codes. Other potentially valuable sources of information within the patent documents, such as the abstract, claims, and description, are not directly utilized for term selection in this specific implementation. Incorporating information from these sections could potentially lead to the identification of more relevant keywords and improved query performance. Finally, the choice of hyperparameters, such as the top-k value (80) for keyword selection, can significantly influence the results. These parameters might not have been optimally tuned, and experimenting with different values could potentially lead to better performance.

当前方法虽然显示出查询优化的自动化水平，但存在一些潜在的局限性。仅使用“或”运算符构建查询的限制，降低了搜索查询的表达能力。专利语言通常涉及不同概念之间的复杂关系，这些关系可能更适合使用“与”或邻近运算符等布尔运算符来捕捉。虽然 TF-IDF 是选择候选关键词的常用技术，但并不总是能识别出专利搜索上下文中语义上最相关的术语。具有高 TF-IDF 得分的术语可能在专利语料库中很常见，但并不一定是特定搜索任务中最具区分性的。如前所述，实现的贪婪优化策略可能会陷入局部最优，无法找到关键词的全局最优组合。将单词添加到查询中的顺序可能会显著影响最终结果。此外， ap50 实现的潜在问题可能导致查询性能评估不准确，阻碍优化过程。 该方法主要关注从标题和 CPC 代码中提取的关键词。在本次实现中，并未直接利用专利文件中的其他可能具有价值的信息来源，如摘要、权利要求和描述，这些部分并未直接用于术语选择。将这些部分的信息纳入可能有助于识别更多相关关键词并提高查询性能。最后，超参数的选择，如关键词选择的 top-k 值（80），对结果有显著影响。这些参数可能并未得到最佳调整，尝试不同的值可能有助于提高性能。  
**6\. Recommendations for Further Improvement**  
**6\. 进一步改进的建议**  
To enhance the effectiveness of the patent query optimization approach, several potential improvements could be explored. Investigating the use of more sophisticated query structures beyond simple "OR" combinations is recommended. Incorporating "AND" operators, phrase matching, and proximity operators could allow for the creation of more precise and targeted queries that better reflect the complex relationships between concepts in patent literature. Exploring the integration of semantic information and more detailed patent classification codes could also be beneficial. Techniques like word embeddings or semantic networks could help capture the semantic similarity between terms, while incorporating IPC (International Patent Classification) codes could provide additional context and improve the retrieval of relevant patents. Experimenting with different optimization algorithms could also yield better results. Revisiting the full simulated annealing implementation with carefully tuned parameters or exploring other metaheuristic algorithms like genetic algorithms or particle swarm optimization might lead to a more effective exploration of the query search space. Refining the term selection methods could also be beneficial. Exploring alternative term weighting schemes beyond TF-IDF, such as BM25, or incorporating terms from other sections of the patent document like the abstract or claims, could lead to a more comprehensive and relevant set of candidate keywords. It is also crucial to verify and correct any potential issues with the implementation of the evaluation metric (ap50) to ensure that the optimization process is guided by accurate performance measurements. Considering the use of other evaluation metrics that might be more suitable for patent search, such as Recall@k or Mean Reciprocal Rank (MRR), could also provide a more comprehensive assessment of the query performance. Exploring query expansion techniques, such as automatically adding synonyms or related terms to the initial set of keywords, could help improve the recall of relevant documents. If user feedback or relevance judgments were available, incorporating this information into the optimization process could further refine the generated queries based on real-world usage. Finally, systematically tuning the hyperparameters of the optimization algorithm, such as the top-k value for keyword selection and the parameters of the annealing schedule (if used), using techniques like cross-validation, could lead to significant improvements in performance.  
为了提高专利查询优化方法的有效性，可以探索几种潜在改进。建议研究使用比简单的“或”组合更复杂的查询结构。结合“与”运算符、短语匹配和邻近运算符可以创建更精确和有针对性的查询，更好地反映专利文献中概念之间的复杂关系。探索整合语义信息和更详细的专利分类代码也可能有益。像词嵌入或语义网络等技术可以帮助捕捉术语之间的语义相似性，而结合国际专利分类（IPC）代码可以提供额外的上下文并改善相关专利的检索。尝试不同的优化算法也可能产生更好的结果。重新审视全模拟退火实现并仔细调整参数，或探索其他元启发式算法，如遗传算法或粒子群优化，可能有助于更有效地探索查询搜索空间。 优化术语选择方法也可能有益。探索超越 TF-IDF 的替代词频加权方案，如 BM25，或结合专利文件其他部分（如摘要或权利要求）中的术语，可能导致更全面和相关的候选关键词集。验证和纠正评估指标（ ap50 ）实施中可能存在的任何潜在问题也至关重要，以确保优化过程由准确的性能测量指导。考虑使用可能更适合专利检索的其他评估指标，如 Recall@k 或平均倒数排名（MRR），也可能提供对查询性能的更全面评估。探索查询扩展技术，例如自动向初始关键词集添加同义词或相关术语，有助于提高相关文档的召回率。如果可用用户反馈或相关性判断，将这些信息纳入优化过程可以进一步根据实际使用情况细化生成的查询。 最后，通过使用交叉验证等技术，系统地调整优化算法的超参数，如关键词选择的 top-k 值和退火计划参数（如果使用），可以显著提高性能。  
**7\. Conclusion  7. 结论**  
The analyzed code presents a framework for automating the optimization of patent search queries using a simulated annealing-inspired approach, which was ultimately implemented as a greedy iterative process. The approach involves preprocessing patent data, creating a search index, selecting relevant keywords based on TF-IDF scores, and then combining these keywords into "OR" queries. While the implemented greedy strategy demonstrates a method for automatically generating queries and achieves a certain level of performance as measured by the AP@50 metric, it also exhibits several limitations, including the restriction to "OR" queries, the reliance on TF-IDF for term selection, and the potential for the greedy approach to get stuck in local optima. Furthermore, the noted potential issue with the evaluation metric raises concerns about the accuracy of the performance measurements. To further enhance the effectiveness of this approach, future work should focus on exploring more sophisticated query structures, incorporating semantic information and detailed patent classification codes, experimenting with different optimization algorithms, refining term selection methods, ensuring the correctness of the evaluation metric, exploring query expansion techniques, and systematically tuning the hyperparameters of the system. By addressing these limitations and exploring these avenues for improvement, the potential of automated query optimization to significantly enhance the efficiency and effectiveness of patent searching can be further realized.  
分析的代码展示了一个利用模拟退火启发式方法自动优化专利搜索查询的框架，最终实现为一个贪婪迭代过程。该方法涉及预处理专利数据，创建搜索索引，根据 TF-IDF 分数选择相关关键词，然后将这些关键词组合成“或”查询。虽然实现的贪婪策略展示了自动生成查询的方法，并通过 AP@50 指标衡量达到了一定的性能水平，但它也显示出一些局限性，包括仅限于“或”查询，依赖于 TF-IDF 进行术语选择，以及贪婪方法可能陷入局部最优的潜在问题。此外，关于评估指标的潜在问题也引发了关于性能测量准确性的担忧。 为进一步提高该方法的有效性，未来的工作应着重于探索更复杂的查询结构，结合语义信息和详细的专利分类代码，尝试不同的优化算法，完善术语选择方法，确保评估指标的准确性，探索查询扩展技术，并系统地调整系统的超参数。通过解决这些限制并探索这些改进途径，自动化查询优化在显著提高专利检索效率和效果方面的潜力可以得到进一步实现。